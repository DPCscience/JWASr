3.1\_Genomic\_Linear\_Mixed\_Model
================
Tianjing Zhao
August 27, 2018

### Install package

``` r
library(devtools)
install_github("zhaotianjing/JWASr")
```

    ## Skipping install of 'JWASr' from a github remote, the SHA1 (df1a8c5a) has not changed since last install.
    ##   Use `force = TRUE` to force installation

### load package

``` r
library("JWASr")
```

    ## 
    ## Attaching package: 'JWASr'

    ## The following object is masked from 'package:base':
    ## 
    ##     assign

### set up (windows user)

``` r
path_libjulia = "C:/Users/ztjsw/AppData/Local/Julia-0.7.0/bin/libjulia.dll"
jwasr_setup_win(path_libjulia)
```

    ## Julia version 0.7.0 at location C:\Users\ztjsw\AppData\Local\JULIA-~2.0\bin will be used.

    ## Loading setup script for JuliaCall...

    ## Finish loading setup script for JuliaCall.

For Mac user, run one line "JWASr::jwasr\_setup()" to setup.

### data

``` r
phenotypes = phenotypes #build-in data

ped_path = "D:\\JWASr\\data\\pedigree.txt"
get_pedigree(ped_path, separator=',', header=T)  #build "pedigree" in Julia
```

    ## JWAS.PedModule.Pedigree(13, Dict{AbstractString,JWAS.PedModule.PedNode}("a2"=>PedNode(1, "0", "0", 0.0),"a1"=>PedNode(2, "0", "0", 0.0),"a7"=>PedNode(4, "a1", "a3", 0.0),"a12"=>PedNode(10, "a9", "a10", 0.1875),"a5"=>PedNode(8, "a1", "a2", 0.0),"a3"=>PedNode(3, "0", "0", 0.0),"a4"=>PedNode(5, "a1", "a2", 0.0),"a6"=>PedNode(6, "a1", "a3", 0.0),"a10"=>PedNode(9, "a5", "a7", 0.125),"a11"=>PedNode(11, "a9", "a10", 0.1875)), Dict(32=>0.25,2=>0.0,16=>0.0,11=>0.5,7=>0.0,9=>0.5,25=>0.375,35=>0.375,43=>0.375,19=>0.5), Set(Any[]), Set(Any[]), Set(Any[]), Set(Any[]))

### build model

``` r
model_equation = "y1 = intercept + x1*x3 + x2 + x3 + ID + dam";
R = 1.0
build_model(model_equation,R) #build "model" in Julia
```

    ## Julia Object of type JWAS.MME.
    ## JWAS.MME(1, AbstractString["y1 = intercept + x1*x3 + x2 + x3 + ID + dam"], JWAS.ModelTerm[ModelTerm(1, "1:intercept", 1, Symbol[:intercept], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries), ModelTerm(1, "1:x1*x3", 2, Symbol[:x1, :x3], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries), ModelTerm(1, "1:x2", 1, Symbol[:x2], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries), ModelTerm(1, "1:x3", 1, Symbol[:x3], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries), ModelTerm(1, "1:ID", 1, Symbol[:ID], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries), ModelTerm(1, "1:dam", 1, Symbol[:dam], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries)], Dict{AbstractString,JWAS.ModelTerm}("1:x3"=>ModelTerm(1, "1:x3", 1, Symbol[:x3], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries),"1:ID"=>ModelTerm(1, "1:ID", 1, Symbol[:ID], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries),"1:dam"=>ModelTerm(1, "1:dam", 1, Symbol[:dam], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries),"1:x2"=>ModelTerm(1, "1:x2", 1, Symbol[:x2], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries),"1:intercept"=>ModelTerm(1, "1:intercept", 1, Symbol[:intercept], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries),"1:x1*x3"=>ModelTerm(1, "1:x1*x3", 2, Symbol[:x1, :x3], AbstractString[], Float64[], 0, Any[], 0, 0×0 SparseMatrixCSC{Float64,Int64} with 0 stored entries)), Symbol[:y1], Symbol[], 0, 0, AbstractString[], 0, 0, 0, 0, 0, [0.0], [0.0], [0.0], JWAS.RandomEffect[], [0.0], 0, 0, 1.0, 1.0, 0, 1, [0.0], [0.0], JWAS.MCMCSamples[], JWAS.DF(4.0, 4.0, 4.0, 4.0), 0, 0, Dict{String,Any}(), 0)

### set covariate

``` r
set_covariate("x1")
```

    ## Julia Object of type Array{Symbol,1}.
    ## Symbol[:x1]

### set random

``` r
G1 = 1.0
set_random("x2", G1)
```

    ## NULL

``` r
G2 = diag(2)
set_random("ID dam", G2, pedigree = TRUE)
```

    ## NULL

### add genotypes

``` r
G3 = 1.0
geno_path = "D:/JWASr/data/genotypes.txt"

add_genotypes(geno_path, G3)  #separator=',' is default
```

### run

``` r
outputMCMCsamples("x2")
```

    ## NULL

``` r
out = runMCMC(data = phenotypes, methods = "BayesC", estimatePi = TRUE, 
                     chain_length = 5000, output_samples_frequency = 100) 
out
```

    ## $`Posterior mean of polygenic effects covariance matrix`
    ##           [,1]      [,2]
    ## [1,] 2.2515918 0.3201589
    ## [2,] 0.3201589 1.7288127
    ## 
    ## $`Posterior mean of marker effects`
    ##      [,1] [,2]      
    ## [1,] "m1" -0.3528505
    ## [2,] "m2" -0.2837558
    ## [3,] "m3" 0.7387871 
    ## [4,] "m4" 0.3174194 
    ## [5,] "m5" 0.3249077 
    ## 
    ## $`Posterior mean of residual variance`
    ## [1] 1.272377
    ## 
    ## $`Posterior mean of marker effects variance`
    ## [1] 0.9280905
    ## 
    ## $`Posterior mean of location parameters`
    ##    Trait    Effect     Level    Estimate
    ## 1      1 intercept intercept    11.22084
    ## 2      1     x1*x3    x1 * m    2.056661
    ## 3      1     x1*x3    x1 * f   0.9054161
    ## 4      1        x2         2   0.2748953
    ## 5      1        x2         1  -0.2128133
    ## 6      1        x3         m   -13.91094
    ## 7      1        x3         f   -13.89881
    ## 8      1        ID        a2   0.2053027
    ## 9      1        ID        a1   0.2089958
    ## 10     1        ID        a3   -0.235355
    ## 11     1        ID        a7  0.04042069
    ## 12     1        ID        a4  -0.3907794
    ## 13     1        ID        a6  -0.1865815
    ## 14     1        ID        a9  -0.2896912
    ## 15     1        ID        a5   0.8834916
    ## 16     1        ID       a10   0.4266387
    ## 17     1        ID       a12  0.07726209
    ## 18     1        ID       a11  0.04771032
    ## 19     1        ID        a8  -0.6744953
    ## 20     1       dam        a2   0.6520092
    ## 21     1       dam        a1  -0.1223837
    ## 22     1       dam        a3  -0.1084323
    ## 23     1       dam        a7 -0.08451849
    ## 24     1       dam        a4   0.2548246
    ## 25     1       dam        a6  -0.4173132
    ## 26     1       dam        a9  0.01880938
    ## 27     1       dam        a5   0.3874218
    ## 28     1       dam       a10   0.1851588
    ## 29     1       dam       a12   0.1197191
    ## 30     1       dam       a11    0.113721
    ## 31     1       dam        a8 -0.03800705
    ## 
    ## $`Posterior mean of Pi`
    ## [1] 0.3757778
